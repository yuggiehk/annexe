
<!DOCTYPE html>
<html lang="en">

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta name="google-site-verification" content="0ahUKEwiTl96jlcL9AhUVN8AKHTMZAuUQPAgI">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>ANNEXE | Yuejiao Su</title>
    <meta name="author" content="Yuejiao Su">
    <meta name="description" content="ANNEXE: Unified Analyzing, Answering, and Pixel Grounding for Egocentric Interaction (CVPR 2025)">
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website">
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet"
        integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css"
        integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css"
        integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css"
        integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css"
        href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media=""
        id="highlight_theme_light">
    <link rel="shortcut icon" href="/assets/img/favicon.ico">
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://yuggiehk.github.io/annexe/">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css"
        media="none" id="highlight_theme_dark">
    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
</head>

<body class="fixed-top-nav sticky-bottom-footer">
    <header>
        <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
            <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span
                        class="font-weight-bold">Yuejiao </span>Su</a> <button class="navbar-toggler collapsed ml-auto"
                    type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav"
                    aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span
                        class="icon-bar bottom-bar"></span> </button>
                <div class="collapse navbar-collapse text-right" id="navbarNav">
                    <ul class="navbar-nav ml-auto flex-nowrap">
                        <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i
                                    class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li>
                    </ul>
                </div>
            </div>
        </nav>
    </header>
    <div class="container mt-5">
        <h2 align="center" class="font-weight-bold"> ANNEXE: Unified Analyzing, Answering, and Pixel Grounding for Egocentric Interaction </h2>
        <p style="text-align: center;" class="font-weight-normal"> CVPR 2025 </p>
        <p style="text-align: center; font-weight: normal"> 
        <a href="https://github.com/yuggiehk/annexe" style="color: #0069d1"
                rel="external nofollow noopener" target="_blank"> Yuejiao Su</a>  
        <a href="https://scholar.google.com/citations?user=MAG909MAAAAJ&hl=en" style="color: #0069d1" rel="external nofollow noopener" target="_blank"> Yi Wang</a> &nbsp
        <a href="" style="color: #0069d1" rel="external nofollow noopener" target="_blank"> Qiongyang Hu</a> &nbsp
        <a href="https://scholar.google.com/citations?user=37S_Zz4AAAAJ&hl=zh-CN" style="color: #0069d1" rel="external nofollow noopener" target="_blank"> Chuang Yang </a> &nbsp 
        <a href="https://scholar.google.com/citations?user=MYREIH0AAAAJ&hl=en" style="color: #0069d1" rel="external nofollow noopener" target="_blank"> Lap-Pui Chau</a> </p>
        <p style="text-align: center; font-weight: normal"> The Hong Kong Polytechnic University  </p>
        <div class="column has-text-centered">
            <div class="publication-links"> 
            <span class="link-block"> 
            <a href="https://arxiv.org/abs/2504.01472" target="_blank" class="external-link button is-normal is-rounded is-dark" rel="external nofollow noopener"> <span class="icon"> <i class="fas fa-file-pdf"></i> </span> <span>Arxiv</span> </a> </span> 
            <span class="link-block"> 
            <a href="https://arxiv.org/abs/2504.01472" target="_blank" class="external-link button is-normal is-rounded is-dark"> <span class="icon"> <i class="fas fa-file-pdf"></i> </span> <span>Supp</span> </a> </span> 
            <span class="link-block"> 
            <a href="https://github.com/yuggiehk/annexe" target="_blank"  class="external-link button is-normal is-rounded is-dark" rel="external nofollow noopener"> <span class="icon"> <i class="fab fa-github"></i> </span> <span>Code</span> </a> </span> 
            <span class="link-block"> 
            <a href="https://github.com/yuggiehk/annexe/blob/main/imgs/poster_cvpr25%E2%80%94annexe.pdf" target="_blank" class="external-link button is-normal is-rounded is-dark" rel="external nofollow noopener"> <span class="icon" style="text-decoration: none"> <i class="fas fa-file-powerpoint"></i> </span> <span>Poster</span> </a> </span> </div>
        </div>
        
<style>
    h3 {
        margin-top: 20px; /* 在标题前添加空白 */
    }
</style>

        <h3> Abstract </h3>
        <p>Egocentric interaction perception is one of the essential branches in investigating human-environment interaction, which lays the basis for developing next-generation intelligent systems. However, existing egocentric interaction understanding methods cannot yield coherent textual and pixel-level responses simultaneously according to user queries, which lacks flexibility for varying downstream application requirements. To comprehend egocentric interactions exhaustively, this paper presents a novel task named Egocentric Interaction Reasoning and pixel Grounding (Ego-IRG). Taking an egocentric image with the query as input, Ego-IRG is the first task that aims to resolve the interactions through three crucial steps: analyzing, answering, and pixel grounding, which results in fluent textual and fine-grained pixel-level responses. Another challenge is that existing datasets cannot meet the conditions for the Ego-IRG task. To address this limitation, this paper creates the Ego-IRGBench dataset based on extensive manual efforts, which includes over 20k egocentric images with 1.6 million queries and corresponding multimodal responses
about interactions. Moreover, we design a unified ANNEXE model to generate text- and pixel-level outputs utilizing multimodal large language models, which enables a comprehensive interpretation of egocentric interactions. The experiments on the Ego-IRGBench exhibit the effectiveness of our ANNEXE model compared with other works.</p>
        <center>
            <figure>
                <div id="projectid"> <img src="https://github.com/yuggiehk/annexe/blob/main/imgs/fig1.png" width="100%"> </div>
                <figcaption style="font-size: 90%; margin-top: 12px; text-align: left; font-weight: 400"> Illuatration of Egocentric Interaction Reasoning and Pixel Grounding (Ego-IRG)</figcaption>
            </figure>
        </center>
        <hr>
        <h3> Pipeline </h3>
        <center>
            <figure>
                <div id="projectid"> <img src="/assets/img/locate/pipeline.png" width="100%"> </div>
                <figcaption style="font-size: 90%; margin-top: 12px; text-align: left; font-weight: 400"> Overview of
                    the proposed LOCATE framework. It achieves part-level knowledge transfer in three steps: 1) locating
                    interaction regions with ψ<sub>cam</sub>, 2) object-part embedding selection with PartSelect, and 3)
                    part-level knowledge transfer with L<sub>cos</sub>. At test time, only the egocentric branch is
                    maintained. </figcaption>
            </figure>
        </center>
        <hr>
        <h3> Video Summary </h3>
        <center>
            <div class="embed-responsive embed-responsive-16by9" style="width: 95%"> <iframe
                    class="embed-responsive-item" src="https://www.youtube.com/embed/RLHansdFxII" frameborder="0"
                    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
                    allowfullscreen=""></iframe> </div>
        </center>
        <hr>
        <h3> Qualitative Results </h3>
        <center>
            <figure>
                <div id="projectid"> <img src="/assets/img/locate/results.png" width="100%"> </div>
                <figcaption style="font-size: 90%; margin-top: 12px; text-align: left; font-weight: 400"> Qualitative
                    comparison between our approach and state-of-the-art affordance grounding methods (<a
                        href="https://arxiv.org/abs/1812.04558" rel="external nofollow noopener"
                        target="_blank">Hotspots [41]</a>, <a href="https://arxiv.org/abs/2203.09905"
                        rel="external nofollow noopener" target="_blank">Cross-view-AG [36]</a>, and <a
                        href="https://arxiv.org/abs/2208.13196" rel="external nofollow noopener"
                        target="_blank">Cross-view-AG+ [35]</a>). For the unseen setting, the displayed objects are not
                    in the training set. For example, the model learns where a motorcycle can be ridden in training, and
                    locates rideable area for the bicycle at test time. </figcaption>
            </figure>
        </center>
        <hr>
        <h3> Citation </h3>
        <div class="language-plaintext highlighter-rouge">
            <div class="highlight">
                <pre class="highlight"><code>@inproceedings{li:locate:2023,
    title = {LOCATE: Localize and Transfer Object Parts for Weakly Supervised Affordance Grounding},
    author = {Li, Gen and Jampani, Varun and Sun, Deqing and Sevilla-Lara, Laura},
    booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
    year={2023}
  }
  </code></pre>
            </div>
        </div>
    </div>
    <footer class="sticky-bottom mt-5">
        <div class="container"> © Copyright 2025 Gen Li. Last updated: May 09, 2025. </div>
    </footer>
    <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js"
        integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js"
        integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js"
        integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js"
        integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
    <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js"
        integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
    <script defer src="/assets/js/zoom.js"></script>
    <script defer src="/assets/js/common.js"></script>
    <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>
    <script async src="https://badge.dimensions.ai/badge.js"></script>
    <script type="text/javascript">window.MathJax = { tex: { tags: "ams" } };</script>
    <script defer type="text/javascript" id="MathJax-script"
        src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
    <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-NYJ88YK0VS"></script>
    <script>function gtag() { window.dataLayer.push(arguments) } window.dataLayer = window.dataLayer || [], gtag("js", new Date), gtag("config", "G-NYJ88YK0VS");</script>
</body>

</html>
